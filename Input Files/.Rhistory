}
gut
find_1_gt <- function(Y){
for (i in 2:length(Y)){
if ((Y[i] > 0.5) & (Y[i-1] < 0.5)){
index <- i
break
} else {
index <- length(Y)
}
}
return(index)
}
find_gut <- function(db){
gut <- c()
breaks <- c(1)
count <- 1
for (i in 2:nrow(db)){
if ((as.integer(db[i,1]) > as.integer(db[(i-1),1])) |
(i == nrow(db))){
db_year <- db[breaks[count]:(i-1),]
year_ind <- find_1_gt(db_year[,2])
lb_ind <- find_1_gt(db_year[,3])
ub_ind <- find_1_gt(db_year[,4])
gut_year <- approx(db_year[(ret_ind-1):ret_ind,2], db_year[(ret_ind-1):ret_ind,1],
xout = 0.5)
gut_lb <- approx(db_year[(lb_ind-1):lb_ind,3], db_year[(lb_ind-1):lb_ind,1],
xout = 0.5)
gut_ub <- approx(db_year[(ub_ind-1):ub_ind,4], db_year[(ub_ind-1):ub_ind,1],
xout = 0.5)
gut <- rbind(gut, c(round(gut_year$y,2), paste("(",round(gut_ub$y,2), ",", round(gut_lb$y,2), ")")))
breaks <- c(breaks, (i))
count <- count + 1
}
}
return(data.frame(gut))
}
model2 <- data.frame(cbind(X, q50[,2], q025[,2], q975[,2]))
colnames(model2) <- c("Year", "mu", "lb", "ub")
gut <- find_gut(model2)
find_1_gt <- function(Y){
for (i in 2:length(Y)){
if ((Y[i] > 0.5) & (Y[i-1] < 0.5)){
index <- i
break
} else {
index <- length(Y)
}
}
return(index)
}
find_gut <- function(db){
gut <- c()
breaks <- c(1)
count <- 1
for (i in 2:nrow(db)){
if ((as.integer(db[i,1]) > as.integer(db[(i-1),1])) |
(i == nrow(db))){
db_year <- db[breaks[count]:(i-1),]
year_ind <- find_1_gt(db_year[,2])
lb_ind <- find_1_gt(db_year[,3])
ub_ind <- find_1_gt(db_year[,4])
gut_year <- approx(db_year[(year_ind-1):year_ind,2], db_year[(year_ind-1):year_ind,1],
xout = 0.5)
gut_lb <- approx(db_year[(lb_ind-1):lb_ind,3], db_year[(lb_ind-1):lb_ind,1],
xout = 0.5)
gut_ub <- approx(db_year[(ub_ind-1):ub_ind,4], db_year[(ub_ind-1):ub_ind,1],
xout = 0.5)
gut <- rbind(gut, c(round(gut_year$y,2), paste("(",round(gut_ub$y,2), ",", round(gut_lb$y,2), ")")))
breaks <- c(breaks, (i))
count <- count + 1
}
}
return(data.frame(gut))
}
model2 <- data.frame(cbind(X, q50[,2], q025[,2], q975[,2]))
colnames(model2) <- c("Year", "mu", "lb", "ub")
gut <- find_gut(model2)
colnames(gut) <- c("Time of GUT", "95% Interval EVI")
plot(X[588:739],Y[588:739], xlab="Date", ylab="EVI")
lines(X[588:739],q50[588:739,2],col=2,lty=1)
lines(X[588:739],q025[588:739,2],col=2,lty=2)
lines(X[588:739],q975[588:739,2],col=2,lty=2)
abline(h=0.5)
for (i in 0:4){
abline(v=gut[nrow(gut)-i,1])
}
gut
gut_year <- data.frame(gut[,1])
col_names(gut_year) <- c("gut_raw")
View(gut_year)
gut_year <- data.frame(gut[,1])
colnames(gut_year) <- c("gut_raw")
gut_year <- gut_year %>%
mutate(year = as.integer(gut_raw)) %>%
mutate(gut_val = gut_raw - year)
gut_year <- data.frame(gut[,1])
colnames(gut_year) <- c("gut_raw")
gut_year <- gut_year %>%
mutate(year = as.integer(gut_raw))
View(gut_year)
gut_year <- gut_year %>%
mutate(gut_val = gut_raw - year)
na.omit(gut_year)
gut_year <- data.frame(gut[,1])
colnames(gut_year) <- c("gut_raw")
gut_year <- na.omit(gut_year)
gut_year <- gut_year %>%
mutate(year = as.integer(gut_raw))
gut_year <- gut_year %>%
mutate(gut_val = gut_raw - year)
View(gut_year)
gut_year$gut_raw - gut_year$year
typeof(gut_year$gut_raw)
gut_year <- data.frame(gut[,1])
colnames(gut_year) <- c("gut_raw")
gut_year <- na.omit(gut_year)
gut_year <- gut_year %>%
mutate(year = as.integer(gut_raw)) %>%
mutate(gut_val = as.float(gut_raw) - year)
gut_year <- data.frame(gut[,1])
colnames(gut_year) <- c("gut_raw")
gut_year <- na.omit(gut_year)
gut_year <- gut_year %>%
mutate(year = as.integer(gut_raw)) %>%
mutate(gut_val = as.numeric(gut_raw) - year)
View(gut_year)
find_1_gt <- function(Y){
for (i in 2:length(Y)){
if ((Y[i] > 0.5) & (Y[i-1] < 0.5)){
index <- i
break
} else {
index <- length(Y)
}
}
return(index)
}
find_gut <- function(db){
gut <- c()
breaks <- c(1)
count <- 1
for (i in 2:nrow(db)){
if ((as.integer(db[i,1]) > as.integer(db[(i-1),1])) |
(i == nrow(db))){
db_year <- db[breaks[count]:(i-1),]
year_ind <- find_1_gt(db_year[,2])
lb_ind <- find_1_gt(db_year[,3])
ub_ind <- find_1_gt(db_year[,4])
gut_year <- approx(db_year[(year_ind-1):year_ind,2], db_year[(year_ind-1):year_ind,1],
xout = 0.5)
gut_lb <- approx(db_year[(lb_ind-1):lb_ind,3], db_year[(lb_ind-1):lb_ind,1],
xout = 0.5)
gut_ub <- approx(db_year[(ub_ind-1):ub_ind,4], db_year[(ub_ind-1):ub_ind,1],
xout = 0.5)
gut <- rbind(gut, c(round(gut_year$y,3), paste("(",round(gut_ub$y,3), ",", round(gut_lb$y,3), ")")))
breaks <- c(breaks, (i))
count <- count + 1
}
}
return(data.frame(gut))
}
model2 <- data.frame(cbind(X, q50[,2], q025[,2], q975[,2]))
colnames(model2) <- c("Year", "mu", "lb", "ub")
gut <- find_gut(model2)
colnames(gut) <- c("Time of GUT", "95% Interval")
plot(X[588:739],Y[588:739], xlab="Date", ylab="EVI")
lines(X[588:739],q50[588:739,2],col=2,lty=1)
lines(X[588:739],q025[588:739,2],col=2,lty=2)
lines(X[588:739],q975[588:739,2],col=2,lty=2)
abline(h=0.5)
for (i in 0:4){
abline(v=gut[nrow(gut)-i,1])
}
gut_year <- data.frame(gut[,1])
colnames(gut_year) <- c("gut_raw")
gut_year <- na.omit(gut_year)
gut_year <- gut_year %>%
mutate(year = as.integer(gut_raw)) %>%
mutate(gut_val = as.numeric(gut_raw) - year)
View(gut_year)
View(gut_year)
gut_year <- data.frame(gut[,1])
colnames(gut_year) <- c("gut_raw")
gut_year <- na.omit(gut_year)
gut_year <- gut_year %>%
mutate(year = as.integer(gut_raw)) %>%
mutate(gut_val = as.numeric(gut_raw) - year)
data <- list(gut = gut_year$gut_val,year=gut$year,n=nrow(gut_year))
model_string <- textConnection("model{
# Likelihood
for(i in 1:n){
gut[i] ~ dnorm(beta1 + beta2*year[i],tau)
}
# Priors
tau   ~  dgamma(0.1, 0.1)
beta1 ~  dnorm(0, 0.001)
beta2 ~  dnorm(0, 0.001)
}")
inits <- list(beta1=rnorm(1),beta2=rnorm(1),tau=10)
model <- jags.model(model_string,data = data, inits=inits, n.chains=2,quiet=TRUE)
gut_year <- data.frame(gut[,1])
colnames(gut_year) <- c("gut_raw")
gut_year <- na.omit(gut_year)
gut_year <- gut_year %>%
mutate(year = as.integer(gut_raw)) %>%
mutate(gut_val = as.numeric(gut_raw) - year)
data <- list(gut = gut_year$gut_val,year=gut$year,n=nrow(gut_year))
model_string <- textConnection("model{
# Likelihood
for(i in 1:n){
gut[i] ~ dnorm(beta1 + beta2*year[i],tau)
}
# Priors
tau   ~  dgamma(0.1, 0.1)
beta1 ~  dnorm(0, 0.001)
beta2 ~  dnorm(0, 0.001)
}")
model <- jags.model(model_string,data = data, inits=inits, n.chains=2,quiet=TRUE)
gut_year <- data.frame(gut[,1])
colnames(gut_year) <- c("gut_raw")
gut_year <- na.omit(gut_year)
gut_year <- gut_year %>%
mutate(year = as.integer(gut_raw)) %>%
mutate(gut_val = as.numeric(gut_raw) - year)
data <- list(gut = gut_year$gut_val, year=gut_year$year,n=nrow(gut_year))
model_string <- textConnection("model{
# Likelihood
for(i in 1:n){
gut[i] ~ dnorm(beta1 + beta2*year[i],tau)
}
# Priors
tau   ~  dgamma(0.1, 0.1)
beta1 ~  dnorm(0, 0.001)
beta2 ~  dnorm(0, 0.001)
}")
model <- jags.model(model_string,data = data, inits=inits, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params  <- c("beta1","beta2")
samples <- coda.samples(model,
variable.names=params,
n.iter=20000, progress.bar="none")
summary(samples)
gut_year <- data.frame(gut[,1])
colnames(gut_year) <- c("gut_raw")
gut_year <- na.omit(gut_year)
gut_year <- gut_year %>%
mutate(year = as.integer(gut_raw)) %>%
mutate(gut_val = as.numeric(gut_raw) - year)
data <- list(gut = gut_year$gut_val, year=gut_year$year,n=nrow(gut_year))
model_string <- textConnection("model{
# Likelihood
for(i in 1:n){
gut[i] ~ dnorm(beta1 + beta2*year[i],tau)
}
# Priors
tau   ~  dgamma(0.1, 0.1)
beta1 ~  dnorm(0, 0.001)
beta2 ~  dnorm(0, 0.001)
}")
model <- jags.model(model_string,data = data, inits=inits, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params  <- c("beta1","beta2")
samples <- coda.samples(model,
variable.names=params,
n.iter=20000, progress.bar="none")
summary(samples)$quantiles
library(dplyr)
library(tibble)
# Load both the failed and not-failed data
failed_df <- read.csv("Failed.csv")
unfailed_df <- read.csv("Not_failed.csv")
# Explore the data: number of unique items
print(n_distinct((failed_df$Pipe_name)))
print(n_distinct((unfailed_df$Pipe_name)))
# Filter the data based on the installation date
failed_sub <- failed_df %>% filter(Install_Year >= 1949)
unfailed_sub <- unfailed_df %>% filter(Install_Year >= 1949)
# Explore the data: number of unique items
print(n_distinct((failed_sub$Pipe_name)))
print(n_distinct((unfailed_sub$Pipe_name)))
# Edit the two datasets to make them homogeneous for combining them together into a single dataset
temp1 <- failed_sub
temp1 <- temp1 %>% add_column(Zone = 999, .after = "Data_type")
temp1 <- temp1 %>% mutate(Y = 1) # add a column to show the pipes have failed
temp2 <- unfailed_sub
temp2 <- temp2 %>% add_column(Year_Reported = 9999, .after = "Zone")
temp2 <- temp2 %>% mutate(Y = 0) # add a column to show the pipes have not failed
# combine the two data
combined <- rbind(temp1,temp2)
# To create dummy variables based on pipe material, compute the count of each pipe material
table(combined$Material)
# Add two columns to take in the dummy values
combined <- combined %>% add_column(CI = 10, .before = "x_coord")
combined <- combined %>% add_column(DI = 10, .after = "CI")
# Create two dummy variables
combined <- combined %>%
mutate(CI = if_else(Material == "CI", 1, 0))
combined <- combined %>%
mutate(DI = if_else(Material == "DI", 1, 0))
# Reduce the size of dataset by keeping only selected number(the same number of data entries as the failed_df) of non-failed pipes
# Select the entries at random
set.seed(123)
subset1 <- combined %>% filter(Y == 1) # all failed pipes
len_subset <- nrow(subset1)
# Create training and testing data from the failed data
# Use 80:20 train to test ratio
#install.packages("sqldf") # uncomment this to install the required packages
library(sqldf)
len1 <- ceiling(0.8 * nrow(subset1))
train <- subset1[sample(nrow(subset1),len1), ]
# subset1 minus train data is the test data
test <- sqldf('SELECT * FROM subset1 EXCEPT SELECT * FROM train')
subset2 <- combined[sample(which(combined$Y == 0),len_subset), ] # subset of non-failed pipes
train <- rbind(train,subset2) # combine the two subsets
train <- train[sample(1:nrow(train)),] # shuffle the data
row.names(train) <- NULL # reset the index
row.names(test) <- NULL # reset the index
X_train <- train[,11:35]
Y_train <- train[,38]
X_test <- test[,11:35]
Y_test <- test[,38]
# Scatter plots of covariates and response
#plot(temp3$Pipe_age, temp3$Y)
#plot(temp3$Diameter, temp3$Y)
#plot(temp3$awc, temp3$Y)
#plot(temp3$organic_matter, temp3$Y)
#plot(temp3$sat_hyd_conductivity_std, temp3$Y)
#plot(temp3$percent_sand, temp3$Y)
#plot(temp3$mean_v, temp3$Y)
#plot(temp3$mean_p, temp3$Y)
#plot(temp3$frost_free_days, temp3$Y)
# drop some of the columns to reduce covariate size
drops <- c("time_to_fail","time_factor","total_failures","ecec","pH","awc",
"organic_matter","sat_hyd_conductivity_std","frost_free_days",
"water_table_depth","aadt","max_p","max_v","direction_change",
"percent_sand","percent_silt","no_prev_fail")
X_train <- X_train[,!(names(X_train) %in% drops)]
X_test <- X_test[,!(names(X_test) %in% drops)]
n <- length(Y_train)
m <- ncol(X_train)
data <- list(Y=Y_train, X=X_train, n=n, m=m)
params <- c("alpha","beta")
burn <- 1000
n.iter <- 1000
thin <- 1
n.chains <- 2
library(rjags)
model_string <- textConnection("model{
#Likelihood
for(i in 1:n){
Y[i] ~ dbern(p[i])
logit(p[i]) <- alpha + inprod(X[i,],beta[])
}
# Priors
for(j in 1:m){
beta[j] ~ dnorm(0,0.001)
}
alpha ~ dnorm(0,0.001)
}")
# Complete run
burn <- 10000
n.iter <- 20000
thin <- 10
n.chains <- 2
model_1 <- jags.model(model_string,data=data,n.chains=n.chains,quiet=TRUE)
update(model, burn, progress.bar="text")
# Complete run
burn <- 10000
n.iter <- 20000
thin <- 10
n.chains <- 2
model_1 <- jags.model(model_string,data=data,n.chains=n.chains,quiet=TRUE)
n <- length(Y_train)
m <- ncol(X_train)
data <- list(Y=Y_train, X=X_train, n=n, m=m)
params <- c("alpha","beta")
burn <- 1000
n.iter <- 1000
thin <- 1
n.chains <- 2
library(rjags)
model_string <- textConnection("model{
#Likelihood
for(i in 1:n){
Y[i] ~ dbern(p[i])
logit(p[i]) <- alpha + inprod(X[i,],beta[])
}
# Priors
for(j in 1:m){
beta[j] ~ dnorm(0,0.001)
}
alpha ~ dnorm(0,0.001)
}")
# Complete run
burn <- 10000
n.iter <- 20000
thin <- 10
n.chains <- 2
model_1 <- jags.model(model_string,data=data,n.chains=n.chains,quiet=TRUE)
update(model_1, burn, progress.bar="text")
samples1 <- coda.samples(model_1, variable.names=params, thin=thin, n.iter=n.iter, progress.bar="text")
#samples2 <- coda.samples(model, variable.names=params, n.iter=n.iter, progress.bar="none")
#plot(samples1)
# Check convergence
gelman.diag(samples1)
autocorr.plot(samples1)
plot(samples1)
# Check convergence
gelman.diag(samples1)
autocorr.plot(samples1)
summary(samples1)
# Check convergence
gelman.diag(samples1)
autocorr.plot(samples1)
View(X_train)
library(rjags)
model_string <- textConnection("model{
#Likelihood
for(i in 1:n){
Y[i] ~ dbern(p[i])
logit(p[i]) <- alpha[] + inprod(X[i,],beta[])
}
# Priors
for(j in 1:m){
alpha[j] ~ dnorm(0,0.001)
beta[j] ~ dnorm(0,0.001)
}
}")
# Complete run
burn <- 5000
n.iter <- 20000
thin <- 10
n.chains <- 2
model_1 <- jags.model(model_string,data=data,n.chains=n.chains,quiet=TRUE)
library(rjags)
model_string <- textConnection("model{
#Likelihood
for(i in 1:n){
Y[i] ~ dbern(p[i])
logit(p[i]) <- alpha[i] + inprod(X[i,],beta[])
alpha[i] ~ dnorm(0,0.001)
}
# Priors
for(j in 1:m){beta[j] ~ dnorm(0,0.001)}
}")
# Complete run
burn <- 5000
n.iter <- 20000
thin <- 10
n.chains <- 2
model_1 <- jags.model(model_string,data=data,n.chains=n.chains,quiet=TRUE)
update(model, burn, progress.bar="text")
# Complete run
burn <- 5000
n.iter <- 20000
thin <- 10
n.chains <- 2
model_1 <- jags.model(model_string,data=data,n.chains=n.chains,quiet=TRUE)
View(data)
library(rjags)
n <- length(Y_train)
m <- ncol(X_train)
data <- list(Y=Y_train, X=X_train, n=n, m=m)
params <- c("alpha","beta")
model_string <- textConnection("model{
#Likelihood
for(i in 1:n){
Y[i] ~ dbern(p[i])
logit(p[i]) <- alpha[i] + inprod(X[i,],beta[])
alpha[i] ~ dnorm(0,0.001)
}
# Priors
for(j in 1:m){beta[j] ~ dnorm(0,0.001)}
}")
# Complete run
burn <- 5000
n.iter <- 20000
thin <- 10
n.chains <- 2
model_1 <- jags.model(model_string,data=data,n.chains=n.chains,quiet=TRUE)
update(model_1, burn, progress.bar="text")
samples1 <- coda.samples(model_1, variable.names=params, thin=thin, n.iter=n.iter, progress.bar="text")
#samples2 <- coda.samples(model, variable.names=params, n.iter=n.iter, progress.bar="none")
summary(samples1)
# Check convergence
gelman.diag(samples1)
install.packages('R6')
install.packages(pathTorCLR.zip, repos = NULL)
install.packages("C:\Users\squar\Downloads\ospsuite_11.0.123", repos = NULL)
install.packages("C:\\Users\\squar\\Downloads\\ospsuite_11.0.123", repos = NULL)
install.packages("C:\Users\squar\Downloads\ospsuite_11.0.123.zip", repos = NULL)
install.packages("C:\\Users\\squar\\Downloads\\ospsuite_11.0.123.zip", repos = NULL)
install.packages("C:\\Users\\squar\\Downloads\\rClr_0.9.1.zip", repos = NULL)
library(ospsuite)
install.packages('stringr')
install.packages('readr')
library(ospsuite)
library(ospsuite)
install.packages('purrr')
library(ospsuite)
install.packages("C:\\Users\\squar\\Downloads\\ospsuite_11.0.123.zip", repos = NULL)
library(ospsuite)
getwd()
setwd("C:/Users/squar/Documents/hydraulic_ABM/Input Files/")
library(bnlearn)
dine <- read.bif('data_driven_dine.bif')
grocery <- read.bif('data_driven_grocery.bif')
wfh <- read.bif('data_driven_wfh.bif')
write.net('data_driven_dine.net', dine)
write.net('data_driven_grocery.net', grocery)
write.net('data_driven_wfh.net', wfh)
