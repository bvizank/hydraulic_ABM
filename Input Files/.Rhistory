for(j in 1:4){beta[j] ~ dnorm(0,0.01)}
}")
data  <- list(Y=Y,X=X,n=length(Y))
model <- jags.model(mod,data = data, n.chains=2,quiet=TRUE)
update(model, 1000, progress.bar="none")
samps <- coda.samples(model, variable.names=c("beta"),
n.iter=10000, progress.bar="none")
samps <- rbind(samps[[1]], samps[[2]])
S <- nrow(samps)
calc_stats <- function(data){
return(c(mean(data),var(data)))
}
D0 <- calc_stats(Y)
D <- matrix(0,S,2)
for (i in 1:S){
beta <- samps[i,]
Yp <- matrix(0,X,1)
for (x in 1:nrow(X)){
Yp[x] <- beta[1] + X[x,1]*beta[2] + X[x,2]*beta[3] + X[x,3]*beta[4]
}
D[i,] <- calc_stats(Yp)
}
names <- c("Mean", "Variance")
for(j in 1:2){
pval <- mean(D[,j]>D0[j])
hist(D[,j],breaks=25,xlim=range(D[,j]) + 1*c(-1,1),
xlab=names[j],
main=paste0("Bayesian p-value = ",round(pval,2)))
abline(v=D0[j],lwd=2,col=2)
}
View(samps)
D0 <- calc_stats(logit(Y))
D0 <- calc_stats(log(Y/(1-Y)))
Y/(1-Y)
expit(D)
exp(D)
D <- matrix(0,S,2)
for (i in 1:S){
beta <- samps[i,]
Yp <- matrix(0,X,1)
for (x in 1:nrow(X)){
Yp[x] <- beta[1] + X[x,1]*beta[2] + X[x,2]*beta[3] + X[x,3]*beta[4]
}
D[i,] <- calc_stats(exp(Yp)/(1+exp(Yp)))
}
D0 <- calc_stats(Y)
names <- c("Mean", "Variance")
for(j in 1:2){
pval <- mean(D[,j]>D0[j])
hist(D[,j],breaks=25,xlim=range(D[,j]) + 1*c(-1,1),
xlab=names[j],
main=paste0("Bayesian p-value = ",round(pval,2)))
abline(v=D0[j],lwd=2,col=2)
}
for (i in 1:S){
beta <- samps[i,]
Yp <- matrix(0,X,1)
for (x in 1:nrow(X)){
Yp[x] <- beta[1] + X[x,1]*beta[2] + X[x,2]*beta[3] + X[x,3]*beta[4]
}
Yp <- exp(Yp)/(1+exp(Yp))
Yp_binary <- matrix(0,X,1)
for (j in 1:nrow(Yp)){
Yp_binary[j] <- rbernoulli(1, p = Yp[j])
}
D[i,] <- calc_stats(exp(Yp)/(1+exp(Yp)))
}
for (i in 1:S){
beta <- samps[i,]
Yp <- matrix(0,X,1)
for (x in 1:nrow(X)){
Yp[x] <- beta[1] + X[x,1]*beta[2] + X[x,2]*beta[3] + X[x,3]*beta[4]
}
Yp <- exp(Yp)/(1+exp(Yp))
Yp_binary <- matrix(0,X,1)
for (j in 1:length(Yp)){
Yp_binary[j] <- rbernoulli(1, p = Yp[j])
}
D[i,] <- calc_stats(exp(Yp)/(1+exp(Yp)))
}
library(purrr)
for (i in 1:S){
beta <- samps[i,]
Yp <- matrix(0,X,1)
for (x in 1:nrow(X)){
Yp[x] <- beta[1] + X[x,1]*beta[2] + X[x,2]*beta[3] + X[x,3]*beta[4]
}
Yp <- exp(Yp)/(1+exp(Yp))
Yp_binary <- matrix(0,X,1)
for (j in 1:length(Yp)){
Yp_binary[j] <- rbernoulli(1, p = Yp[j])
}
D[i,] <- calc_stats(exp(Yp)/(1+exp(Yp)))
}
names <- c("Mean", "Variance")
for(j in 1:2){
pval <- mean(D[,j]>D0[j])
hist(D[,j],breaks=25,xlim=range(D[,j]) + 1*c(-1,1),
xlab=names[j],
main=paste0("Bayesian p-value = ",round(pval,2)))
abline(v=D0[j],lwd=2,col=2)
}
for (i in 1:S){
beta <- samps[i,]
Yp <- matrix(0,X,1)
for (x in 1:nrow(X)){
Yp[x] <- beta[1] + X[x,1]*beta[2] + X[x,2]*beta[3] + X[x,3]*beta[4]
}
Yp <- exp(Yp)/(1+exp(Yp))
Yp_binary <- matrix(0,X,1)
for (j in 1:length(Yp)){
Yp_binary[j] <- rbernoulli(1, p = Yp[j])
}
D[i,] <- calc_stats(Yp_binary)
}
names <- c("Mean", "Variance")
for(j in 1:2){
pval <- mean(D[,j]>D0[j])
hist(D[,j],breaks=25,xlim=range(D[,j]) + 1*c(-1,1),
xlab=names[j],
main=paste0("Bayesian p-value = ",round(pval,2)))
abline(v=D0[j],lwd=2,col=2)
}
summary(Y)
summary(as.factor(Y))
summary(as.factor(Y))[1]
library(rjags)
library(geoR)
library(purrr)
Y   <- gambia$pos
X   <- gambia[,4:8]
X   <- scale(X)
mod <- textConnection("model{
for(i in 1:n){
Y[i] ~ dbern(pi[i])
logit(pi[i]) <- beta[1] + X[i,1]*beta[2] +
X[i,2]*beta[3] + X[i,3]*beta[4] +
X[i,4]*beta[5] + X[i,5]*beta[6]
}
for(j in 1:6){beta[j] ~ dnorm(0,0.01)}
}")
data  <- list(Y=Y,X=X,n=length(Y))
model <- jags.model(mod,data = data, n.chains=2,quiet=TRUE)
update(model, 5000, progress.bar="none")
samps <- coda.samples(model, variable.names=c("beta"),
n.iter=10000, progress.bar="none")
samps <- rbind(samps[[1]], samps[[2]])
S <- nrow(samps)
calc_stats <- function(data){
return(c(mean(data),var(data),summary(as.factor(data))[1]))
}
D0 <- calc_stats(Y)
D <- matrix(0,S,3)
for (i in 1:S){
beta <- samps[i,]
Yp <- matrix(0,X,1)
for (x in 1:nrow(X)){
Yp[x] <- beta[1] + X[x,1]*beta[2] + X[x,2]*beta[3] + X[x,3]*beta[4]
}
Yp <- exp(Yp)/(1+exp(Yp))
Yp_binary <- matrix(0,X,1)
for (j in 1:length(Yp)){
Yp_binary[j] <- rbernoulli(1, p = Yp[j])
}
D[i,] <- calc_stats(Yp_binary)
}
names <- c("Mean", "Variance", "Number w/o Malaria")
for(j in 1:3){
pval <- mean(D[,j]>D0[j])
hist(D[,j],breaks=25,xlim=range(D[,j]) + 2*c(-1,1),
xlab=names[j],
main=paste0("Bayesian p-value = ",round(pval,2)))
abline(v=D0[j],lwd=2,col=2)
}
Y   <- gambia$pos
X   <- gambia[,c(4,5,7)]
X   <- scale(X)
mod <- textConnection("model{
for(i in 1:n){
Y[i] ~ dbern(pi[i])
logit(pi[i]) <- beta[1] + X[i,1]*beta[2] +
X[i,2]*beta[3] + X[i,3]*beta[4]
}
for(j in 1:4){beta[j] ~ dnorm(0,0.01)}
}")
data  <- list(Y=Y,X=X,n=length(Y))
model <- jags.model(mod,data = data, n.chains=2,quiet=TRUE)
update(model, 1000, progress.bar="none")
samps <- coda.samples(model, variable.names=c("beta"),
n.iter=10000, progress.bar="none")
samps <- rbind(samps[[1]], samps[[2]])
S <- nrow(samps)
calc_stats <- function(data){
return(c(mean(data),var(data),summary(as.factor(data))[1]))
}
D0 <- calc_stats(Y)
D <- matrix(0,S,3)
for (i in 1:S){
beta <- samps[i,]
Yp <- matrix(0,X,1)
for (x in 1:nrow(X)){
Yp[x] <- beta[1] + X[x,1]*beta[2] + X[x,2]*beta[3] + X[x,3]*beta[4]
}
Yp <- exp(Yp)/(1+exp(Yp))
Yp_binary <- matrix(0,X,1)
for (j in 1:length(Yp)){
Yp_binary[j] <- rbernoulli(1, p = Yp[j])
}
D[i,] <- calc_stats(Yp_binary)
}
names <- c("Mean", "Variance", "Number w/o Malaria")
for(j in 1:3){
pval <- mean(D[,j]>D0[j])
hist(D[,j],breaks=25,xlim=range(D[,j]) + 2*c(-1,1),
xlab=names[j],
main=paste0("Bayesian p-value = ",round(pval,2)))
abline(v=D0[j],lwd=2,col=2)
}
x = seq(0,1,0.01)
y = runif(length(x), 0, 0.000001)
plot(x, y)
y = dunif(length(x), 0, 0.000001)
plot(x, y)
y = punif(length(x), 0, 0.000001)
plot(x, y)
y = dunif(x, 0, 0.000001)
plot(x, y)
y = dunif(x, 0.99999,1)
plot(x, y)
knitr::opts_chunk$set(fig.pos = "!h", out.extra = "", echo = TRUE, warning = FALSE, message = FALSE)
evi_db <- read.csv('EVI_Data.csv')
wf_db <- read.csv("All_Payment_Methods052222.csv")
ch_db <- read.csv("Chase PDM Jan - May 2022.csv")
ch_db <- read.csv("Chase PDM Jan - May 2022.CSV")
wf_db <- read.csv("All_Payment_Methods052222.csv")
setwd("~/Documents/hydraulic_ABM/Input Files")
library(bnlearn)
read.bif("pmt_ppe.bif")
fitted <- read.bif("pmt_ppe.bif")
fitted.nodes
#### clear the environment ####
rm(list = ls())
cat("\014")
#### Change working directory ####
setwd("C:/Users/squar/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
# setwd("/Users/vizan/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
#### global variables ####
test_groups <- c('demo','media','trust','cultcog','personal','finite')
country_list <- c('IT', 'UK', 'US', 'AU', 'DE', 'ES', 'MX', 'SE', 'JP', 'KR', 'CN')
selection_proc = 'FS'       # either FS for forward selection or BE for backward elimination
survey_date = 1             # either 1 or 2 for march or april
rds_path <- paste0('./tmp_database.RDS')
# start a timer
ptm <- proc.time()
# Call all functions and install necessary packages ---------
# package installation
source(file = './package_installation.R')
# load all written functions
source(file = './functions.R')
# Read dataset -------------
db <- read.xlsx('./data_file_reduced.xlsx')
# remove descriptive rows
db <- db[-c(1:3),]
# predictors
db_x <- data.frame(lapply(db[,1:110], as.factor))
db_x <- subset(db_x, Survey_round == 1)
names(db_x)[names(db_x) == "Ethnic.min"] = "Ethnicmin"
setwd("C:/Users/squar/Documents/hydraulic_ABM/Input Files/")
dine <- read.bif('/pmt_models/dine_out_less_pmt-6.bif')
grocery <- read.bif('/pmt_models/shop_groceries_less_pmt-6.bif')
ppe <- read.bif('/data_driven_models/mask.bif')
wfh <- read.bif('/data_driven_models/work_from_home.bif')
all_nodes <- c(nodes(dine), nodes(wfh), nodes(grocery), nodes(ppe))
all_nodes <- all_nodes[!all_nodes %in% c("dine_out_less", "work_from_home", "shop_groceries_less", "mask")]
all_nodes <- all_nodes[!duplicated(all_nodes)]
abm_out <- db_x[all_nodes]
abm_out <- cleaning(abm_out)
write.csv(abm_out, 'all_bbn_data.csv')
#### clear the environment ####
rm(list = ls())
cat("\014")
#### Change working directory ####
setwd("C:/Users/squar/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
# setwd("/Users/vizan/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
#### global variables ####
test_groups <- c('demo','media','trust','cultcog','personal','finite')
country_list <- c('IT', 'UK', 'US', 'AU', 'DE', 'ES', 'MX', 'SE', 'JP', 'KR', 'CN')
selection_proc = 'FS'       # either FS for forward selection or BE for backward elimination
survey_date = 1             # either 1 or 2 for march or april
rds_path <- paste0('./tmp_database.RDS')
# start a timer
ptm <- proc.time()
# Call all functions and install necessary packages ---------
# package installation
source(file = './package_installation.R')
# load all written functions
source(file = './functions.R')
# Read dataset -------------
db <- read.xlsx('./data_file_reduced.xlsx')
# remove descriptive rows
db <- db[-c(1:3),]
# predictors
db_x <- data.frame(lapply(db[,1:110], as.factor))
db_x <- subset(db_x, Survey_round == 1)
names(db_x)[names(db_x) == "Ethnic.min"] = "Ethnicmin"
setwd("C:/Users/squar/Documents/hydraulic_ABM/Input Files/")
dine <- read.bif('./pmt_models/dine_out_less_pmt-6.bif')
grocery <- read.bif('./pmt_models/shop_groceries_less_pmt-6.bif')
ppe <- read.bif('./data_driven_models/mask.bif')
wfh <- read.bif('./data_driven_models/work_from_home.bif')
all_nodes <- c(nodes(dine), nodes(wfh), nodes(grocery), nodes(ppe))
all_nodes <- all_nodes[!all_nodes %in% c("dine_out_less", "work_from_home", "shop_groceries_less", "mask")]
all_nodes <- all_nodes[!duplicated(all_nodes)]
abm_out <- db_x[all_nodes]
abm_out <- cleaning(abm_out)
write.csv(abm_out, 'all_bbn_data.csv')
#### clear the environment ####
rm(list = ls())
cat("\014")
#### Change working directory ####
setwd("C:/Users/squar/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
# setwd("/Users/vizan/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
#### global variables ####
test_groups <- c('demo','media','trust','cultcog','personal','finite')
country_list <- c('IT', 'UK', 'US', 'AU', 'DE', 'ES', 'MX', 'SE', 'JP', 'KR', 'CN')
selection_proc = 'FS'       # either FS for forward selection or BE for backward elimination
survey_date = 1             # either 1 or 2 for march or april
rds_path <- paste0('./tmp_database.RDS')
# start a timer
ptm <- proc.time()
# Call all functions and install necessary packages ---------
# package installation
source(file = './package_installation.R')
# load all written functions
source(file = './functions.R')
# Read dataset -------------
db <- read.xlsx('./data_file_reduced.xlsx')
# remove descriptive rows
db <- db[-c(1:3),]
# predictors
db_x <- data.frame(lapply(db[,1:110], as.factor))
db_x <- subset(db_x, Survey_round == 1)
names(db_x)[names(db_x) == "Ethnic.min"] = "Ethnicmin"
setwd("C:/Users/squar/Documents/hydraulic_ABM/Input Files/")
dine <- read.bif('./pmt_models/dine_out_less_pmt-6.bif')
grocery <- read.bif('./pmt_models/shop_groceries_less_pmt-6.bif')
ppe <- read.bif('./data_driven_models/mask.bif')
wfh <- read.bif('./data_driven_models/work_from_home.bif')
all_nodes <- c(nodes(dine), nodes(wfh), nodes(grocery), nodes(ppe))
all_nodes <- all_nodes[!all_nodes %in% c("dine_out_less", "work_from_home", "shop_groceries_less", "mask")]
all_nodes <- all_nodes[!duplicated(all_nodes)]
abm_out <- db_x[all_nodes]
abm_out <- cleaning(abm_out)
write.csv(abm_out, 'all_bbn_data.csv')
nodes(dine)
all_nodes
View(abm_out)
#### clear the environment ####
rm(list = ls())
cat("\014")
#### Change working directory ####
setwd("C:/Users/squar/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
# setwd("/Users/vizan/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
#### global variables ####
test_groups <- c('demo','media','trust','cultcog','personal','finite')
country_list <- c('IT', 'UK', 'US', 'AU', 'DE', 'ES', 'MX', 'SE', 'JP', 'KR', 'CN')
selection_proc = 'FS'       # either FS for forward selection or BE for backward elimination
survey_date = 1             # either 1 or 2 for march or april
rds_path <- paste0('./tmp_database.RDS')
# start a timer
ptm <- proc.time()
# Call all functions and install necessary packages ---------
# package installation
source(file = './package_installation.R')
# load all written functions
source(file = './functions.R')
# Read dataset -------------
db <- read.xlsx('./data_file_reduced.xlsx')
# remove descriptive rows
db <- db[-c(1:3),]
# predictors
db_x <- data.frame(lapply(db[,1:110], as.factor))
db_x <- subset(db_x, Survey_round == 1)
names(db_x)[names(db_x) == "Ethnic.min"] = "Ethnicmin"
setwd("C:/Users/squar/Documents/hydraulic_ABM/Input Files/")
dine <- read.bif('./pmt_models/dine_out_less_pmt-6.bif')
grocery <- read.bif('./pmt_models/shop_groceries_less_pmt-6.bif')
ppe <- read.bif('./data_driven_models/mask.bif')
wfh <- read.bif('./data_driven_models/work_from_home.bif')
all_nodes <- c(nodes(dine), nodes(wfh), nodes(grocery), nodes(ppe))
all_nodes <- all_nodes[!all_nodes %in% c("dine_out_less", "work_from_home", "shop_groceries_less", "mask")]
all_nodes <- all_nodes[!duplicated(all_nodes)]
abm_out <- db_x[all_nodes]
abm_out <- cleaning(abm_out, threshold=0.12)
write.csv(abm_out, 'all_bbn_data.csv')
#### clear the environment ####
rm(list = ls())
cat("\014")
#### Change working directory ####
setwd("C:/Users/squar/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
# setwd("/Users/vizan/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
#### global variables ####
test_groups <- c('demo','media','trust','cultcog','personal','finite')
country_list <- c('IT', 'UK', 'US', 'AU', 'DE', 'ES', 'MX', 'SE', 'JP', 'KR', 'CN')
selection_proc = 'FS'       # either FS for forward selection or BE for backward elimination
survey_date = 1             # either 1 or 2 for march or april
rds_path <- paste0('./tmp_database.RDS')
# start a timer
ptm <- proc.time()
# Call all functions and install necessary packages ---------
# package installation
source(file = './package_installation.R')
# load all written functions
source(file = './functions.R')
# Read dataset -------------
db <- read.xlsx('./data_file_reduced.xlsx')
# remove descriptive rows
db <- db[-c(1:3),]
# predictors
db_x <- data.frame(lapply(db[,1:110], as.factor))
db_x <- subset(db_x, Survey_round == 1)
names(db_x)[names(db_x) == "Ethnic.min"] = "Ethnicmin"
setwd("C:/Users/squar/Documents/hydraulic_ABM/Input Files/")
dine <- read.bif('./pmt_models/dine_out_less_pmt-6.bif')
grocery <- read.bif('./pmt_models/shop_groceries_less_pmt-6.bif')
ppe <- read.bif('./data_driven_models/mask.bif')
wfh <- read.bif('./data_driven_models/work_from_home.bif')
all_nodes <- c(nodes(dine), nodes(wfh), nodes(grocery), nodes(ppe))
all_nodes <- all_nodes[!all_nodes %in% c("dine_out_less", "work_from_home", "shop_groceries_less", "mask")]
all_nodes <- all_nodes[!duplicated(all_nodes)]
abm_out <- db_x[all_nodes]
abm_out <- cleaning(abm_out, threshold=0.12)
write.csv(abm_out, 'all_bbn_data.csv')
nodes(ppe)
nodes(groc)
nodes(grocery)
#### clear the environment ####
rm(list = ls())
cat("\014")
#### Change working directory ####
setwd("C:/Users/squar/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
# setwd("/Users/vizan/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
#### global variables ####
test_groups <- c('demo','media','trust','cultcog','personal','finite')
country_list <- c('IT', 'UK', 'US', 'AU', 'DE', 'ES', 'MX', 'SE', 'JP', 'KR', 'CN')
selection_proc = 'FS'       # either FS for forward selection or BE for backward elimination
survey_date = 1             # either 1 or 2 for march or april
rds_path <- paste0('./tmp_database.RDS')
# start a timer
ptm <- proc.time()
# Call all functions and install necessary packages ---------
# package installation
source(file = './package_installation.R')
# load all written functions
source(file = './functions.R')
# Read dataset -------------
db <- read.xlsx('./data_file_reduced.xlsx')
# remove descriptive rows
db <- db[-c(1:3),]
# predictors
db_x <- data.frame(lapply(db[,1:110], as.factor))
db_x <- subset(db_x, Survey_round == 1)
names(db_x)[names(db_x) == "Ethnic.min"] = "Ethnicmin"
setwd("C:/Users/squar/Documents/hydraulic_ABM/Input Files/")
dine <- read.bif('./pmt_models/dine_out_less_pmt-6.bif')
grocery <- read.bif('./pmt_models/shop_groceries_less_pmt-6.bif')
ppe <- read.bif('./data_driven_models/mask.bif')
wfh <- read.bif('./data_driven_models/work_from_home.bif')
all_nodes <- c(nodes(dine), nodes(wfh), nodes(grocery), nodes(ppe))
all_nodes <- all_nodes[!all_nodes %in% c("dine_out_less", "work_from_home", "shop_groceries_less", "mask")]
all_nodes <- all_nodes[!duplicated(all_nodes)]
abm_out <- db_x[all_nodes]
abm_out <- cleaning(abm_out, threshold=0.12)
write.csv(abm_out, 'all_bbn_data.csv')
#### clear the environment ####
rm(list = ls())
cat("\014")
#### Change working directory ####
setwd("C:/Users/squar/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
# setwd("/Users/vizan/OneDrive - North Carolina State University/Research/Code/BBN/country_breakdown")
#### global variables ####
test_groups <- c('demo','media','trust','cultcog','personal','finite')
country_list <- c('IT', 'UK', 'US', 'AU', 'DE', 'ES', 'MX', 'SE', 'JP', 'KR', 'CN')
selection_proc = 'FS'       # either FS for forward selection or BE for backward elimination
survey_date = 1             # either 1 or 2 for march or april
rds_path <- paste0('./tmp_database.RDS')
# start a timer
ptm <- proc.time()
# Call all functions and install necessary packages ---------
# package installation
source(file = './package_installation.R')
# load all written functions
source(file = './functions.R')
# Read dataset -------------
db <- read.xlsx('./data_file_reduced.xlsx')
# remove descriptive rows
db <- db[-c(1:3),]
# predictors
db_x <- data.frame(lapply(db[,1:110], as.factor))
db_x <- subset(db_x, Survey_round == 1)
names(db_x)[names(db_x) == "Ethnic.min"] = "Ethnicmin"
setwd("C:/Users/squar/Documents/hydraulic_ABM/Input Files/")
dine <- read.bif('./pmt_models/dine_out_less_pmt-6.bif')
grocery <- read.bif('./pmt_models/shop_groceries_less_pmt-6.bif')
ppe <- read.bif('./data_driven_models/mask.bif')
wfh <- read.bif('./data_driven_models/work_from_home.bif')
all_nodes <- c(nodes(dine), nodes(wfh), nodes(grocery), nodes(ppe))
all_nodes <- all_nodes[!all_nodes %in% c("dine_out_less", "work_from_home", "shop_groceries_less", "mask")]
all_nodes <- all_nodes[!duplicated(all_nodes)]
abm_out <- db_x[all_nodes]
abm_out <- cleaning(abm_out, threshold=0.12)
write.csv(abm_out, 'all_bbn_data.csv')
